{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import random\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lingyaoli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lingyaoli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lingyaoli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# clean tweet data\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "# prefer to create own stopword list\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    text = ''.join([digit for digit in text if not digit.isdigit()])\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    text = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split() if word not in stopword)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"&#39;\", \"'\")\n",
    "    text = text.replace(\"&quot;\", \"\")\n",
    "    text = text.replace(\"&amp;\", \"\")\n",
    "    text = text.replace(\"‘\", \"'\")\n",
    "    text = text.replace(\"’\", \"'\")\n",
    "    text = text.replace(\"“\", \"'\")\n",
    "    text = text.replace(\"”\", \"'\")\n",
    "    text = text.replace(\" – \", \" \")\n",
    "    text = text.replace(\"—\", \" \")\n",
    "    text = text.replace(\"•\", \" \")\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'bit.ly/\\S+', '', text)\n",
    "    text = re.sub('(RT\\s@[A-Za-z0-9-_]+[A-Za-z0-9-_]: +)', '', text)\n",
    "    text = re.sub('(@[A-Za-z0-9-_]+[A-Za-z0-9-_]+)', '', text)\n",
    "    text = BeautifulSoup(text, \"lxml\").text \n",
    "    text = text.lower()\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_emojis(text)\n",
    "    #text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_text(text):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))  \n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:            \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @latimes: Today's earthquake was the larges...</td>\n",
       "      <td>1675</td>\n",
       "      <td>3</td>\n",
       "      <td>today earthquake be the large in southern cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KTLA: Check out this fissure near the site...</td>\n",
       "      <td>1087</td>\n",
       "      <td>2</td>\n",
       "      <td>check out this fissure near the site of last n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @AP: An earthquake with a preliminary magni...</td>\n",
       "      <td>822</td>\n",
       "      <td>0</td>\n",
       "      <td>an earthquake with a preliminary magnitude of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @WCKitchen: Major damage is expected here i...</td>\n",
       "      <td>743</td>\n",
       "      <td>2</td>\n",
       "      <td>major damage be expect here in ridgecrest cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Will4Planet: Ridgecrest Earthquake before ...</td>\n",
       "      <td>664</td>\n",
       "      <td>2</td>\n",
       "      <td>ridgecrest earthquake before th july after th ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  count  label  \\\n",
       "0  RT @latimes: Today's earthquake was the larges...   1675      3   \n",
       "1  RT @KTLA: Check out this fissure near the site...   1087      2   \n",
       "2  RT @AP: An earthquake with a preliminary magni...    822      0   \n",
       "3  RT @WCKitchen: Major damage is expected here i...    743      2   \n",
       "4  RT @Will4Planet: Ridgecrest Earthquake before ...    664      2   \n",
       "\n",
       "                                          clean_text  \n",
       "0  today earthquake be the large in southern cali...  \n",
       "1  check out this fissure near the site of last n...  \n",
       "2  an earthquake with a preliminary magnitude of ...  \n",
       "3  major damage be expect here in ridgecrest cali...  \n",
       "4  ridgecrest earthquake before th july after th ...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read original training data\n",
    "\n",
    "dfr1 = pd.read_excel('training_2.xlsx')\n",
    "dfr1['clean_text'] = dfr1['text'].apply(clean_text)\n",
    "print(len(dfr1))\n",
    "dfr1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196\n",
      "1190\n",
      "1184\n",
      "1251\n",
      "4821\n"
     ]
    }
   ],
   "source": [
    "# read augmented training data\n",
    "\n",
    "dff1 = pd.read_csv('earthquake_1_aug.txt', sep='\\t', names=[\"label\",\"clean_text\"])\n",
    "print(len(dff1))\n",
    "dff2 = pd.read_csv('earthquake_2_aug.txt', sep='\\t', names=[\"label\",\"clean_text\"])\n",
    "print(len(dff2))\n",
    "dff3 = pd.read_csv('earthquake_3_aug.txt', sep='\\t', names=[\"label\",\"clean_text\"])\n",
    "print(len(dff3))\n",
    "dff0 = pd.read_csv('earthquake_0_aug.txt', sep='\\t', names=[\"label\",\"clean_text\"])\n",
    "print(len(dff0))\n",
    "df1 = pd.concat([dff1, dff2, dff3, dff0])\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13f2d2400>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['3', '2', '1', '0']\n",
    "plt.figure(figsize=(10,4))\n",
    "df1['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume before filter 4821\n",
      "volume after filter 4821\n"
     ]
    }
   ],
   "source": [
    "def filter_text(text):\n",
    "    return None if len(str(text).split()) < 2 else text\n",
    "\n",
    "print('volume before filter', len(df1))\n",
    "df1['clean_text'] = df1['clean_text'].apply(filter_text)\n",
    "df1 = df1.dropna(subset=['clean_text'])\n",
    "print('volume after filter', len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An earthquake rattled a large swath of Souther...</td>\n",
       "      <td>0</td>\n",
       "      <td>an earthquake rattle a large swath of southern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California earthquake brings scattered damage\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>california earthquake brings scatter damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No significant damage or injuries were found i...</td>\n",
       "      <td>1</td>\n",
       "      <td>no significant damage or injury be find in los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earthquake broke my phone :(</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake break my phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#coppednews Southern California earthquake tri...</td>\n",
       "      <td>2</td>\n",
       "      <td>coppednews southern california earthquake trig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  An earthquake rattled a large swath of Souther...      0   \n",
       "1  California earthquake brings scattered damage\\...      1   \n",
       "2  No significant damage or injuries were found i...      1   \n",
       "3                       earthquake broke my phone :(      1   \n",
       "4  #coppednews Southern California earthquake tri...      2   \n",
       "\n",
       "                                          clean_text  \n",
       "0  an earthquake rattle a large swath of southern...  \n",
       "1        california earthquake brings scatter damage  \n",
       "2  no significant damage or injury be find in los...  \n",
       "3                          earthquake break my phone  \n",
       "4  coppednews southern california earthquake trig...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read testing data\n",
    "\n",
    "df2 = pd.read_excel('testing_2.xlsx')\n",
    "df2['clean_text'] = df2['text'].apply(clean_text)\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df1['clean_text']\n",
    "y_train = df1['label']\n",
    "X_test = df2['clean_text']\n",
    "y_test = df2['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Forest + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.81%\n",
      "test accuracy: 84.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.75      0.92      0.83       197\n",
      "           2       0.79      0.53      0.63       136\n",
      "           1       0.92      0.92      0.92       448\n",
      "           0       0.44      0.37      0.40        19\n",
      "\n",
      "    accuracy                           0.84       800\n",
      "   macro avg       0.72      0.69      0.69       800\n",
      "weighted avg       0.84      0.84      0.84       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(n_estimators=105, max_depth=45, random_state=66)),\n",
    "              ])\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "y_trad = rf.predict(X_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"train accuracy: %.2f%%\" % (accuracy_score(y_trad, y_train)*100))\n",
    "print(\"test accuracy: %.2f%%\" % (accuracy_score(y_pred, y_test)*100))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multinomial Bayes + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 92.10%\n",
      "test accuracy: 78.38%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.65      0.87      0.74       197\n",
      "           2       0.58      0.55      0.56       136\n",
      "           1       0.94      0.83      0.89       448\n",
      "           0       0.75      0.32      0.44        19\n",
      "\n",
      "    accuracy                           0.78       800\n",
      "   macro avg       0.73      0.64      0.66       800\n",
      "weighted avg       0.80      0.78      0.79       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb = nb.fit(X_train, y_train)\n",
    "\n",
    "y_trad = nb.predict(X_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(\"train accuracy: %.2f%%\" % (accuracy_score(y_trad, y_train)*100))\n",
    "print(\"test accuracy: %.2f%%\" % (accuracy_score(y_pred, y_test)*100))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Classification + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 98.98%\n",
      "test accuracy: 82.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.81      0.85      0.83       197\n",
      "           2       0.65      0.60      0.63       136\n",
      "           1       0.91      0.89      0.90       448\n",
      "           0       0.36      0.53      0.43        19\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.68      0.72      0.69       800\n",
      "weighted avg       0.83      0.82      0.82       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVC(penalty='l2', loss='hinge', multi_class='crammer_singer', max_iter=5000))\n",
    "               ])\n",
    "svc = svc.fit(X_train, y_train)\n",
    "\n",
    "y_trad = svc.predict(X_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"train accuracy: %.2f%%\" % (accuracy_score(y_trad, y_train)*100))\n",
    "print(\"test accuracy: %.2f%%\" % (accuracy_score(y_pred, y_test)*100))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression (multi-nomial) + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.94%\n",
      "test accuracy: 81.88%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.82      0.81      0.81       197\n",
      "           2       0.63      0.55      0.59       136\n",
      "           1       0.89      0.92      0.90       448\n",
      "           0       0.38      0.53      0.44        19\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.68      0.70      0.69       800\n",
      "weighted avg       0.82      0.82      0.82       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(solver='lbfgs', C=1e5, multi_class='multinomial', max_iter=3000))\n",
    "               ])\n",
    "lr = lr.fit(X_train, y_train)\n",
    "\n",
    "y_trad = lr.predict(X_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"train accuracy: %.2f%%\" % (accuracy_score(y_trad, y_train)*100))\n",
    "print(\"test accuracy: %.2f%%\" % (accuracy_score(y_pred, y_test)*100))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
